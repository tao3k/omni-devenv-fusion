# System Layering Architecture (Trinity System Layers)

> **Status**: Active | **Version**: v4.0
> **Philosophy**: "Thin Agent, Fat Kernel, Rust Foundation"
> **Current Agent Persona**: [Omega Architecture](../human/architecture/omega-architecture.md)

The system is organized into **four strict software layers**, referred to as the **Trinity System Layers**. This structure ensures high performance, clear separation of concerns, and massive scalability.

## ğŸ—ï¸ Layer 1: Foundation (The Bedrock)

**Responsibility**: I/O, Performance, Macros, Type System.

- **Implementation**: `packages/python/foundation/`
- **Key Features**: Orjson-powered serialization, `@skill_command` decorators, Pydantic V2 schemas.

## ğŸ§  Layer 2: Core (The Kernel)

**Responsibility**: Business Logic, State Management, Skill Loading.

- **Implementation**: `packages/python/core/`
- **Key Features**: `Kernel` singleton, `ScriptLoader`, `Router & Sniffer`.

## ğŸ”Œ Layer 3: MCP-Server (The Transport)

**Responsibility**: Protocol Implementation, Transport Layer.

- **Implementation**: `packages/python/mcp-server/`
- **Key Features**: JSON-RPC over STDIO/SSE, high-performance serialization.

## ğŸ¯ Layer 4: Agent (The Interface)

**Responsibility**: Protocol Adaptation, User Interaction, CLI Entry Points.

- **Implementation**: `packages/python/agent/`
- **Key Features**: `OmniLoop`, `OmegaRunner`, CLI commands.

---

## Architecture Evolution: Roles vs Layers

It is important to distinguish between **Agent Roles** (How the agent thinks) and **System Layers** (How the code is structured).

| Concept           | Previous (v1.0)                               | Current (v3.0+)                                          |
| :---------------- | :-------------------------------------------- | :------------------------------------------------------- |
| **Agent Roles**   | Trinity Roles (Orchestrator, Coder, Executor) | **Omega Architecture** (Cortex, Cerebellum, Hippocampus) |
| **System Layers** | Flat structure                                | **Trinity System Layers** (Foundation, Core, MCP, Agent) |

The **Omega Architecture** sits on top of the **Trinity System Layers** to provide an autonomous, self-evolving agent experience.

### Script Loader Integration

```python
from omni.core.skills.script_loader import ScriptLoader
from omni.foundation.api.decorators import get_script_config

loader = ScriptLoader(scripts_path="assets/skills/git/scripts", skill_name="git")
loader.load_all()

# Commands are automatically registered from @skill_command decorators
for cmd in loader.commands.values():
    config = get_script_config(cmd)  # Reads V2 config
    print(f"{cmd.__name__}: {config['category']}")
```

### Event Reactor (v5.0 - The Grand Integration)

The Kernel integrates with the Rust Event Bus for reactive architecture:

```python
# In Kernel._on_ready()
from omni.core.kernel.reactor import get_reactor, EventTopic

# Initialize reactor
self._reactor = get_reactor()

# Wire Cortex to file events (auto-increment indexing)
self._reactor.register_handler(
    EventTopic.FILE_CHANGED,
    self._on_file_changed_cortex,
    priority=10
)

# Wire Sniffer to file events (reactive context detection)
self.sniffer.register_to_reactor()

# Start consumer loop
await self._reactor.start()
```

---

## ğŸ”Œ Layer 3: MCP-Server (The Transport)

**Responsibility:** Protocol Implementation, Transport Layer.

### Key Components

| Component          | Purpose                                  |
| ------------------ | ---------------------------------------- |
| **JSON-RPC Types** | Inherit from Foundation `OrjsonModel`    |
| **StdioTransport** | orjson-powered stdin/stdout (zero-copy)  |
| **SSEServer**      | orjson-powered HTTP streaming            |
| **stdio_server()** | MCP SDK-compatible async context manager |

### MCP Types (Inherit from Foundation)

```python
from omni.mcp.types import JSONRPCRequest, JSONRPCResponse
from omni.foundation.api.types import OrjsonModel

# All types inherit OrjsonModel for 10x faster serialization
assert issubclass(JSONRPCRequest, OrjsonModel)
assert issubclass(JSONRPCResponse, OrjsonModel)

# Direct orjson output
request = JSONRPCRequest(method="tools/list", params={})
json_bytes = request.model_dump_json_bytes()  # Ultra-fast!
```

### STDIO Transport (MCP SDK Compatible)

```python
from omni.mcp.transport.stdio import stdio_server

# MCP SDK-compatible API
async with stdio_server() as (read_stream, write_stream):
    await server.run(read_stream, write_stream, init_options)
```

---

## ğŸ¯ Layer 4: Agent (The Interface)

**Responsibility:** Protocol Adaptation (MCP), User Interaction, CLI Entry Points.

### Key Components

| Component            | Purpose                                          |
| -------------------- | ------------------------------------------------ |
| **mcp_server.stdio** | Uses `omni.mcp.transport.stdio.stdio_server`     |
| **mcp_server.sse**   | Uses `omni.mcp.transport.sse.SseServerTransport` |
| **CLI**              | Command-line entry points                        |

### Agent Integration

```python
# packages/python/agent/src/omni/agent/mcp_server/stdio.py
from omni.mcp.transport.stdio import stdio_server

async def run_stdio() -> None:
    """Run server in stdio mode for Claude Desktop."""
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream, get_init_options())
```

---

## ğŸ“ Directory Structure

```
omni-dev-fusion/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ foundation/          # Layer 1: Foundation (I/O, protocols, macros)
â”‚   â”‚   â”‚   â””â”€â”€ omni/foundation/
â”‚   â”‚   â”‚       â”œâ”€â”€ api/
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ decorators.py    # @skill_command macro
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ types.py         # OrjsonModel, CommandResult[T]
â”‚   â”‚   â”‚       â””â”€â”€ config/              # Settings, logging, paths
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ core/                # Layer 2: Core (Kernel, Skills)
â”‚   â”‚   â”‚   â””â”€â”€ omni/core/
â”‚   â”‚   â”‚       â”œâ”€â”€ kernel/      # Kernel singleton, lifecycle
â”‚   â”‚   â”‚       â”œâ”€â”€ skills/      # ScriptLoader, extensions
â”‚   â”‚   â”‚       â””â”€â”€ router/      # Router & Sniffer
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ mcp-server/          # Layer 3: MCP Transport
â”‚   â”‚   â”‚   â””â”€â”€ omni/mcp/
â”‚   â”‚   â”‚       â”œâ”€â”€ types.py     # JSON-RPC types (inherits OrjsonModel)
â”‚   â”‚   â”‚       â”œâ”€â”€ server.py    # MCPServer orchestration
â”‚   â”‚   â”‚       â””â”€â”€ transport/   # StdioTransport, SSEServer, stdio_server()
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ agent/               # Layer 4: Agent (Thin adapter)
â”‚   â”‚       â””â”€â”€ omni/agent/
â”‚   â”‚           â”œâ”€â”€ mcp_server/  # Uses omni.mcp.transport
â”‚   â”‚           â”œâ”€â”€ cli/         # CLI entry points
â”‚   â”‚           â””â”€â”€ core/        # Layer 5: Context Optimization (Token Diet)
â”‚   â”‚               â””â”€â”€ context/ # ContextPruner, ContextManager, Turn
â”‚   â”‚
â”‚   â””â”€â”€ rust/
â”‚       â””â”€â”€ crates/
â”‚           â”œâ”€â”€ omni-core-rs/    # Rust core (bindings)
â”‚           â””â”€â”€ omni-scanner/    # Rust index generator
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ skills/                  # Skill definitions
â”‚       â”œâ”€â”€ git/
â”‚       â”‚   â”œâ”€â”€ SKILL.md
â”‚       â”‚   â”œâ”€â”€ rules.toml       # Declarative sniffer rules
â”‚       â”‚   â”œâ”€â”€ scripts/         # @skill_command decorated functions
â”‚       â”‚   â””â”€â”€ extensions/      # Skill extensions
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ skill_index.json             # Single Source of Truth (generated by Rust)
â””â”€â”€ pyproject.toml               # Project config
```

---

## ğŸš€ Performance Characteristics

### Serialization Benchmarks

| Operation             | Standard json | orjson | Speedup  |
| --------------------- | ------------- | ------ | -------- |
| `model_dump()`        | ~62ms         | ~35ms  | **1.8x** |
| Complex nested data   | ~120ms        | ~40ms  | **3x**   |
| MCP message roundtrip | ~200ms        | ~50ms  | **4x**   |

### Memory Efficiency

- **Zero-copy reading**: StdioTransport reads directly from `stdin.buffer`
- **Binary output**: orjson.dumps returns `bytes` (no encoding overhead)
- **Compact serialization**: orjson produces smaller JSON output

---

## ğŸ”„ Data Flow: The Complete Stack

### Build Time

```
Rust Scanner â†’ assets/skills/ â†’ skill_index.json
```

### Boot Time

```
1. Agent starts â†’ Boots Kernel
2. Kernel initializes DiscoveryService â†’ Reads skill_index.json (O(1))
3. Kernel initializes IntentSniffer â†’ Loads rules from Index
4. Kernel loads ScriptLoader for each skill
   â””â”€â”€ ScriptLoader reads _skill_config from @skill_command decorators
```

### Run Time (Complete Stack)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L4: Agent (Claude Desktop / Claude Code CLI)                    â”‚
â”‚  stdin/stdout â—„â”€â”€â”€â”€â”€â”€ JSON-RPC Messages â”€â”€â”€â”€â”€â”€â–º                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: MCP-Server (omni.mcp.transport)                             â”‚
â”‚  stdio_server() [orjson.loads/dumps]                            â”‚
â”‚  SSEServer [orjson-powered streaming]                           â”‚
â”‚  JSON-RPC Types (OrjsonModel)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: Core (omni.core)                                            â”‚
â”‚  ScriptLoader reads _skill_config from @skill_command           â”‚
â”‚  Kernel orchestrates skill execution                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  | KernelReactor (Event-Driven Architecture v5.0)             |  â”‚
â”‚  |                                                            |  â”‚
â”‚  |  Rust GLOBAL_BUS â”€â”€â–º KernelReactor â”€â”€â–º Handlers            |  â”‚
â”‚  |  (tokio broadcast)    (async loop)     Cortex/Sniffer      |  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: Foundation (omni.foundation)                                â”‚
â”‚  @skill_command generates input_schema (Pydantic V2)            â”‚
â”‚  CommandResult[T] with @computed_field                          â”‚
â”‚  OrjsonModel for 10x fast serialization                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### Event Flow (v5.0 - The Grand Integration)

```

File Watcher (Rust) Agent Loop (Python) Sniffer (Python)
â”‚ â”‚ â”‚
â–¼ â–¼ â”‚
GLOBAL_BUS.publish GLOBAL_BUS.publish KernelReactor
(file/changed) (agent/step_complete) (FILE_CREATED)
â”‚ â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KernelReactor â”‚
â”‚ (async consumer) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ â”‚
â–¼ â–¼ â–¼
\_on_file_changed_cortex AsyncPersistenceService \_on_file_changed
â”‚ â”‚ â”‚
â–¼ â–¼ â–¼
Indexer.index_file() Queue â†’ Worker â†’ Store sniff(parent_dir)

```

### MCP Message Flow (STDIO)

```

stdin.buffer (bytes)
â†“
orjson.loads() â† L3: omni.mcp.transport.stdio.stdio_server
â†“
JSON-RPC Request
â†“
L4: agent.mcp_server.stdio.run_stdio()
â†“
L3: MCPServer.run()
â†“
L2: ScriptLoader.get_command("git.status")
â†“
L1: @skill_command decorated function executes
â†“
Returns CommandResult[T] (inherits OrjsonModel)
â†“
orjson.dumps() â† L3: omni.mcp.transport.stdio
â†“
stdout.buffer (bytes)

````

---

## ğŸ¯ Key Components

### Foundation Decorators

```python
from omni.foundation.api import skill_command, get_script_config

@skill_command(
    name="git_status",
    category="read",
    inject_root=True,
    cache_ttl=60.0
)
def status(project_root: Path) -> CommandResult[dict]:
    """Get git status."""
    ...

# Config is auto-generated at decoration time
config = get_script_config(status)
# {
#     "name": "git_status",
#     "category": "read",
#     "input_schema": {...},  # Pydantic V2 generated
#     "execution": {...}
# }
````

### Kernel (`omni.core.kernel`)

```python
from omni.core.kernel import get_kernel

kernel = get_kernel()
await kernel.initialize()
await kernel.start()

context = kernel.skill_context
skills = context.list_skills()
```

### ScriptLoader

```python
from omni.core.skills.script_loader import ScriptLoader

loader = ScriptLoader(
    scripts_path="assets/skills/git/scripts",
    skill_name="git"
)
loader.load_all()

# Commands are auto-registered from @skill_command decorators
cmd = loader.commands["git.status"]
```

### Intent Sniffer (`omni.core.router.sniffer`)

```python
from omni.core.router.sniffer import IntentSniffer

sniffer = IntentSniffer()
sniffer.load_from_index()  # Load rules from skill_index.json
skills = sniffer.sniff("/project")  # Returns matching skill names
```

---

## ğŸ“œ Sniffer Rules (rules.toml)

Skills define activation rules in `extensions/sniffer/rules.toml`:

```toml
[[rule]]
skill = "python"
type = "file_exists"
pattern = "pyproject.toml"

[[rule]]
skill = "python"
type = "file_pattern"
pattern = "*.py"

[[rule]]
skill = "git"
type = "file_exists"
pattern = ".git"
```

---

## ğŸ”¥ Hot Reload Workflow

1. **Watcher:** Kernel watches `skill_index.json` and `scripts/*.py`
2. **Update:** Developer modifies skill script
3. **Signal:** Kernel detects file change â†’ reloads ScriptLoader
4. **Notification:** Agent sends `notifications/tools/list_changed`
5. **Refresh:** Client re-fetches tools list

---

## Migration Checklist (v2.0 â†’ v4.0)

| Old Pattern                | New Pattern                                          |
| -------------------------- | ---------------------------------------------------- |
| Manual `input_schema` dict | Auto-generated by `@skill_command`                   |
| `json.dumps()`             | `OrjsonModel.model_dump_json_bytes()`                |
| `mcp.server.stdio`         | `omni.mcp.transport.stdio.stdio_server`              |
| `mcp.server.sse`           | `omni.mcp.transport.sse.SseServerTransport`          |
| Pydantic V1                | Pydantic V2 (`model_json_schema`, `@computed_field`) |
| Dataclass                  | `OrjsonModel` base class                             |
| `agent.skills.*`           | `omni.core.skills.script_loader`                     |
| Raw message list           | `ContextManager` with smart pruning                  |

---

## ğŸ¯ Context Optimization (The Token Diet)

**Philosophy**: "Keep what matters, prune what doesn't."

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L4: Agent (omni_loop.py)                                        â”‚
â”‚  ContextManager.get_active_context(strategy="pruned")          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L5: Context Optimization (NEW - Token Diet)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ContextPruner  â”‚    â”‚ ContextManager â”‚    â”‚ Turn Tracking  â”‚ â”‚
â”‚  â”‚ - System Keep  â”‚    â”‚ - add_turn()   â”‚    â”‚ - Snapshot     â”‚ â”‚
â”‚  â”‚ - Recent Keep  â”‚    â”‚ - get_context()â”‚    â”‚ - Stats        â”‚ â”‚
â”‚  â”‚ - Overflow     â”‚    â”‚ - prune()      â”‚    â”‚ - Serializationâ”‚ â”‚
â”‚  â”‚ - Segment      â”‚    â”‚ - compress()   â”‚    â”‚ - Summary      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: MCP-Server (omni.mcp.transport)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

```python
# packages/python/agent/src/omni/agent/core/context/

from .pruner import ContextPruner, PruningConfig
from .manager import ContextManager

# Initialize with token budget
config = PruningConfig(max_tokens=128000, retained_turns=10)
ctx = ContextManager(pruner=ContextPruner(config))

# Add conversation turns
ctx.add_turn("User message", "Assistant response")

# Get pruned context for LLM
messages = ctx.get_active_context(strategy="pruned")

# Stats
stats = ctx.stats()
# {'turn_count': 1, 'total_messages': 3, 'estimated_tokens': ~150, ...}
```

### Pruning Strategy (The "Token Diet")

| Layer        | Priority | Action                             |
| ------------ | -------- | ---------------------------------- |
| **System**   | CRITICAL | Always preserved (identity, tools) |
| **Recent**   | HIGH     | Last N turns kept intact           |
| **Summary**  | MEDIUM   | Optional insertion point           |
| **Overflow** | LOW      | Truncated from oldest              |

### Smart Context Compression

When conversation history exceeds limits, instead of discarding old messages, the system can semantically compress them:

```python
from omni.agent.core.context import ContextManager

ctx = ContextManager()

# Segment messages into 3 parts
system, to_summarize, recent = ctx.segment()

# Async compression with NoteTaker integration
await ctx.compress()  # Returns True if compression occurred

# Summary is stored and reused
print(ctx.summary)  # Persisted summary text

# Summary is included in context
messages = ctx.get_active_context(strategy="pruned")
# System messages + [Context Summary] + Recent messages
```

**Compression Flow:**

1. `segment()` splits messages into (system, to_summarize, recent)
2. `compress()` formats old messages into trajectory structure
3. Calls NoteTaker `summarize()` to generate markdown summary
4. Extracts key content and stores in `self.summary`
5. Old messages replaced with summary in system prompts

**Fallback:** If NoteTaker unavailable, uses simple extractive summarization.

### Related Files

**Python (Context Layer):**

- `packages/python/agent/src/omni/agent/core/context/pruner.py`
- `packages/python/agent/src/omni/agent/core/context/manager.py`
- `packages/python/agent/src/omni/agent/core/omni.py`
- `packages/python/agent/tests/unit/test_context/`

---

## ğŸ“š Related Documentation

### Architecture Guides

- [Zero-Code Skill Architecture](./zero-code-skill-architecture.md)
- [MCP Core Architecture](../developer/mcp-core-architecture.md)
- [Hippocampus](../human/architecture/hippocampus.md) - Memory Interface (long-term memory)

### Feature Guides

- [MCP Transport Layer](./mcp-transport.md)
- [Script Loader](./script-loader.md)
- [Context Optimization (Token Diet)](./context-optimization.md)
- [Vector Index Optimization](./vector-index.md)
- [Rust-Python Bridge](./rust-python-bridge.md)
- [Permission Gatekeeper (Zero Trust)](./permission-gatekeeper.md)
- [Immune System](./immune-system.md)

### Reference

- [ODF-EP Protocol](../reference/odf-ep-protocol.md)
- [Extension System](../reference/extension-system.md)
