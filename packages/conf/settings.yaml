# packages/conf/settings.yaml
# Project Settings - System Defaults
#
# System default lives here. User overrides (merged on top):
#   $PRJ_CONFIG_HOME/omni-dev-fusion/settings.yaml
# (e.g. .config/omni-dev-fusion/settings.yaml or --conf <dir>)
#
# Usage:
#   omni ...                      # uses default conf dir (.config)
#   omni --conf ./.config.dev ... # uses custom conf dir
#
#   from omni.foundation.config.settings import get_setting
#   cog_path = get_setting("config.cog_toml")
#
# Note: Actual commit types/scopes are read from cog.toml/.conform.yaml,
# not from this file. This file only defines the paths to those files.

# =============================================================================
# Configuration Directory
# =============================================================================
# Logical config directory key (used by legacy consumers).
# User override path is still controlled by CLI flag: --conf /path/to/conf

conf_dir: "assets"

# =============================================================================
# Asset Directories (Migration: agent/ -> assets/)
# =============================================================================
# All paths are relative to project root
assets:
  # Skills directory
  skills_dir: "assets/skills"

  # Templates directory (Cascading template loading)
  # User overrides > Skill defaults pattern
  templates_dir: "assets/templates"

  # Knowledge base (RAG documents)
  knowledge_dir: "assets/knowledge"

  # How-to guides
  howto_dir: "assets/how-to"

  # Specifications
  specs_dir: "assets/specs"
  specs_archive_dir: "assets/specs.archive"
  specs_template: "assets/specs/template.md"

  # Standards and guidelines
  standards_dir: "assets/standards"

  # Writing style
  writing_style_dir: "assets/writing-style"

  # Core prompts
  prompts_dir: "assets/prompts"

  # Repomix cache
  cache_dir: ".cache"

# =============================================================================
# Configuration Files
# =============================================================================
config:
  # Commit message validation configuration files
  cog_toml: "cog.toml"
  conform_yaml: ".conform.yaml"
  lefhook_yaml: "lefthook.yml"
  # Project root marker
  project_marker: ".git"

# =============================================================================
# API Key Configuration
# =============================================================================
# API key files - paths relative to project root (git toplevel)
api:
  # Anthropic API key configuration file
  anthropic_settings: ".claude/settings.json"

# =============================================================================
# Inference Configuration (LLM API)
# =============================================================================
# Configure which LLM provider to use for internal inference (writer, semantic search, etc.)
inference:
  # Provider name for litellm (e.g., "anthropic", "openai", "azure", "minimax")
  # Use "minimax" for MiniMax API
  provider: "minimax"

  # Environment variable name for the API key
  # For MiniMax: "MINIMAX_API_KEY"
  api_key_env: "MINIMAX_API_KEY"

  # API base URL (not needed for minimax - LiteLLM handles it internally)
  base_url: null

  # Default model name (MiniMax model)
  model: "MiniMax-M2.5"

  # Request timeout in seconds
  timeout: 120

  # Max tokens per response
  max_tokens: 4096

# =============================================================================
# MCP Configuration
# =============================================================================
# MCP configuration files
mcp:
  # Main MCP configuration file
  config_file: ".mcp.json"

  # Server names
  orchestrator: "orchestrator"
  coder: "coder"
  executor: "executor"

  # Default timeout (seconds)
  timeout: 120

# =============================================================================
# Git Commit Settings
# =============================================================================
# Note: Actual types and scopes are loaded from cog.toml/.conform.yaml
# These are fallback values only when config files don't exist

commit:
  # Default protocol: "stop_and_ask" or "auto_commit"
  protocol: "stop_and_ask"

# =============================================================================
# Logging Settings
# =============================================================================
logging:
  format: "json"
  level: "info"

# =============================================================================
# Configuration-Driven Context
# =============================================================================
prompts:
  core_path: "assets/prompts/system_core.md"
  user_custom_path: ".cache/user_custom.md"

# =============================================================================
# Knowledge Ingestion Configuration
# =============================================================================
knowledge:
  # Knowledge base directory
  base_dir: "assets/knowledge"
  # How-to guides directory
  howto_dir: "assets/how-to"
  # Specific knowledge files
  gitops_file: "assets/how-to/gitops.md"
  gitops_cache: "assets/knowledge/gitops-cache.md"
  testing_workflows: "assets/how-to/testing-workflows.md"

# =============================================================================
# Standards and References
# =============================================================================
standards:
  # Feature lifecycle document
  feature_lifecycle: "assets/standards/feature-lifecycle.md"
  # Documentation workflow guide
  documentation_workflow: "assets/how-to/documentation-workflow.md"

# =============================================================================
# Spec Templates
# =============================================================================
specs:
  # Spec template file
  template: "assets/specs/template.md"
  # Example spec path for documentation
  example: "assets/specs/auth_module.md"

# =============================================================================
# Memory Configuration (The Hippocampus)
# =============================================================================
# Memory path follows prj-spec: {git_toplevel}/.cache/{project}/.memory/
# Override with custom path if needed
memory:
  # Custom memory path (leave empty to use default prj-spec)
  # path: "/custom/path/.memory"
  path: ""

# =============================================================================
# Embedding Configuration
# =============================================================================
# Configure embedding provider for vector storage (RAG, memory, skills)
# Auto-detection: First MCP loads model + starts server, others auto-connect as clients
embedding:
  # Provider: empty=auto-detect (default), "client"=force HTTP client, "fallback"=hash only
  # Override with: provider: "client" or provider: "fallback"
  provider: ""

  # Local model: Qwen3-Embedding-0.6B (1024 dimensions, lightweight for fast loading)
  # Alternatives: Qwen/Qwen3-Embedding-4B (2560 dimensions, higher quality)
  model: "Qwen/Qwen3-Embedding-0.6B"

  # Embedding dimension (1024 for Qwen3-Embedding-0.6B, 2560 for Qwen3-Embedding-4B)
  dimension: 1024

  # Auto rebuild skills/router vector indexes when embedding model or dimension changes.
  auto_reindex_on_change: true

  # HTTP server port
  http_port: 18501

  # Memory optimization settings
  # device: "auto"=detect (MPS on Apple Silicon), "cpu", "mps"
  device: "auto"
  # torch_dtype: "auto", "float32", "float16" (float16 only effective with MPS)
  torch_dtype: "float16"
  # quantize: null=disabled, "int8"=8-bit, "int4"=4-bit (requires bitsandbytes)
  quantize: null
  # truncate_dim: null=disabled, 256/512/768=truncate to target dimension
  # MRL (Matryoshka): reduces output by 75% (1024->256), speeds up ~4x
  # WARNING: Lower dimension = slight precision loss. 256 recommended for speed.
  truncate_dim: 256

  # HTTP client URL (only needed when forcing client mode)
  # client_url: "http://127.0.0.1:18501"

# =============================================================================
# RAG Module Configuration
# =============================================================================
# Configuration for RAG-Anything integration with modular feature flags
# Each capability can be independently enabled/disabled

rag:
  enabled: true

  # Phase 1: Document Parsing (Foundation - always enabled if rag.enabled)
  document_parsing:
    enabled: true
    parser: "docling" # docling, mineru, auto
    max_workers: 4
    show_progress: true

  # Phase 2: Multimodal Processing (Optional - requires vision model)
  multimodal:
    enabled: false
    vision_model: "gpt-4o" # or local VLM
    table_processor: "auto"
    equation_processor: "auto"
    ocr_enabled: true

  # Phase 3: Knowledge Graph (Core - Rust optimized)
  knowledge_graph:
    enabled: true
    entity_types:
      - "PERSON"
      - "ORGANIZATION"
      - "CONCEPT"
      - "PROJECT"
      - "TOOL"
      - "SKILL"
    extraction_llm: null # null = use default LLM
    store_in_rust: true
    max_entities_per_doc: 100
    relation_types:
      - "WORKS_FOR"
      - "PART_OF"
      - "USES"
      - "DEPENDS_ON"
      - "SIMILAR_TO"

  # Phase 4: Rust Search Enhancements
  rust_search:
    enabled: true
    entity_aware: true
    rerank: true
    hybrid_v2: true
    rrf_k: 60

# =============================================================================
# Checkpoint / Workflow State Configuration
# =============================================================================
# Vector Store (LanceDB) Configuration
# =============================================================================
# Shared by router (skills), knowledge, and other vector-backed features
# Bounded defaults below prevent MCP/long-lived process memory growth (e.g. 24GB+).
# For ~1–2G total RSS (minimal embedding model): use 134217728 (128 MiB) and max_cached_tables: 2–4.
vector:
  # LanceDB index cache size in bytes. Bounded default keeps MCP memory under control.
  # Set to null to use runtime default (256 MiB). Use 134217728 (128 MiB) for ~1–2G target.
  index_cache_size_bytes: 134217728 # 128 MiB (was 256; lower for minimal-model 1–2G RSS)

  # Max in-memory dataset (table) count; LRU eviction when exceeded. Prevents unbounded growth.
  # Set to null to use runtime default (8). Use 2 or 4 for ~1–2G total memory.
  max_cached_tables: 4

  # Default partition column for add_documents_partitioned when partition_by is not passed.
  # Used for fragment-aligned writes (e.g. "skill_name" or "category"). Leave null to require explicit partition_by.
  default_partition_column: "skill_name"

# =============================================================================
# Unified checkpoint storage using Rust LanceDB backend
# All workflow state persists here - no separate SQLite per workflow

checkpoint:
  # LanceDB database path (relative to project root or absolute)
  # All LanceDB data is consolidated under .cache/omni-vector/
  # Default: .cache/omni-vector/checkpoints.lance
  db_path: ".cache/omni-vector/checkpoints.lance"

  # Table prefix for different workflow types
  # smart_commit -> table: checkpoint_smart_commit
  table_prefix: "checkpoint_"

  # Default dimension for embeddings (should match embedding.dimension above)
  embedding_dimension: 1024

# =============================================================================
# Cache Configuration
# =============================================================================
cache:
  # Project name for cache directory structure
  # All cache dirs will be under: {git_root}/.cache/{project_name}/
  project_name: "omni-dev-fusion"

# =============================================================================
# Skill Management
# =============================================================================
skills:
  # =============================================================================
  # Core Preload (Base Layer) - Loaded in ALL modes
  # =============================================================================
  # Keep this minimal to reduce LLM context noise
  preload:
    - knowledge
    - memory
    - git
    - code_tools
    - skill

  # =============================================================================
  # CLI / Omni Run Extensions
  # =============================================================================
  # Skills to ADD when running via 'omni run' (CLI mode).
  # This inherits from 'preload' and adds the tools below.
  cli:
    extend:
      - writer
      - testing_protocol

  # =============================================================================
  # Filter Commands (Pattern Matching)
  # =============================================================================
  # Use glob patterns to hide tools from the LLM (MCP Server).
  # Syntax:
  #   "pattern"   -> Filter matching commands
  #   "!pattern"  -> Whitelist (Keep these even if they match a filter)
  filter_commands:
    # --- Terminal: Hide noisy ops, keep safe ones for LLM ---
    - "terminal.*" # Filter ALL terminal commands
    # - "!terminal.run_task"        # Exception: Allow task runner

    # - "!filesystem.apply_changes"  # Exception: Keep batch operations

    # --- Git: Hide raw plumbing commands ---
    - "git.raw_*" # Hide internal raw commands

  # LRU + TTL eviction settings
  ttl:
    timeout: 1800 # Skill eviction timeout in seconds (30 minutes)
    check_interval: 300 # TTL cleanup check interval (5 minutes)

  max_loaded: 15 # Maximum loaded skills limit

  # Reload behavior
  reload:
    enabled: true
    auto_confirm: false

  # =============================================================================
  # Dynamic Tool Loading Limits
  # =============================================================================
  limits:
    dynamic_tools: 100 # Max dynamic tools per request
    core_min: 3 # Minimum guaranteed tools
    rerank_threshold: 20 # If tools found > this, re-rank by score
    schema_cache_ttl: 300 # Tool schema cache TTL (seconds)
    auto_optimize: true # Enable automatic context optimization

  # =============================================================================
  # Skill Architecture Standards (ODF-EP v7.0)
  # =============================================================================
  # Defines the canonical structure for all Omni skills.
  # Aligned with Anthropic's Skill Standard but extended for Omni's MCP features.
  # Used by StructureValidator to enforce skill structure integrity.

  architecture:
    version: "v2.0"

    # Source of truth for skill definition
    definition_file: "SKILL.md"

    # Directory structure specification
    structure:
      # Required files for every skill (ODF-EP v7.0 Core)
      required:
        # Core definition (Trinity: State)
        - path: "SKILL.md"
          description: "Skill metadata (YAML frontmatter) and system prompts"
          type: "file"

      # Default files/directories (auto-generated if not exists)
      # Validation ignores these - they can exist or not
      default:
        - path: "scripts/"
          description: "Standalone executables (Python workflows, state management)"
          type: "dir"

        # Extensions directory: Stateful modules, lifecycle hooks, complex logic (Meta-Agent, Watchers)
        - path: "extensions/"
          description: "Internal modules and plugins (Meta-Agent, Factory, Watchers)"
          type: "dir"

        - path: "templates/"
          description: "Jinja2 templates for skill output (Cascading loader)"
          type: "dir"

        - path: "references/"
          description: "Markdown documentation for RAG ingestion"
          type: "dir"

        - path: "assets/"
          description: "Static resources, templates, guides"
          type: "dir"

        - path: "data/"
          description: "Data files (JSON, CSV, etc.) for skill operations"
          type: "dir"

        - path: "tests/"
          description: "Pytest tests specifically for this skill"
          type: "dir"

        - path: "repomix.json"
          description: "Repomix configuration (auto-generated if missing)"
          type: "file"

        # Sidecar Execution Pattern support
        - path: "pyproject.toml"
          description: "Skill dependencies for subprocess mode (crawl4ai, playwright, etc.)"
          type: "file"

        - path: "README.md"
          description: "Developer documentation (human-readable, not for LLM)"
          type: "file"

      # Optional structure examples (for documentation and validation)
      # These show common patterns but are not required
      optional:
        # Example: Cascading template structure
        - path: "templates/"
          description: "Skill-local Jinja2 templates (defaults)"
          example: |
            assets/skills/git/templates/
            ├── commit_message.j2
            ├── workflow_result.j2
            └── error_message.j2
          validation: "Optional - if present, enables cascading template loading"

        # Example: Scripts directory with atomic implementations
        - path: "scripts/"
          description: "Atomic implementations (Router-Controller pattern)"
          example: |
            assets/skills/git/scripts/
            ├── __init__.py
            ├── rendering.py    # Template rendering layer
            ├── workflow.py     # Git workflow logic
            └── status.py       # Git status implementation
          validation: "Optional - recommended for complex skills"

        # Example: User template overrides directory
        - path: "assets/templates/{skill}/"
          description: "User-level template overrides (cascading pattern)"
          example: |
            assets/templates/git/
            ├── commit_message.j2  # Overrides skill default
            └── workflow_result.j2
          validation: "Optional - if exists, takes precedence over skill defaults"

    # Jinja2 template configuration
    templates:
      # Centralized templates directory
      source_dir: "assets/templates/skill"

      # Auto-generate templates
      auto_generate:
        - source: "SKILL.md.j2"
          target: "SKILL.md"
        - source: "scripts/commands.py.j2"
          target: "scripts/commands.py"

    # Validation rules
    validation:
      # Block commit if skill structure is invalid
      block_on_invalid: true

      # Allow non-standard files (warning only)
      allow_ghost_files: false

      # Disallowed files (must not exist - causes LLM confusion)
      disallowed_files:
        - "state.py" # Legacy - inline state in scripts/commands.py

  # =============================================================================
  # Configuration-Driven Tool Aliasing (Native Mimicry)
  # =============================================================================
  # Defines how skill commands are presented to Claude (aliasing and behavioral hints).
  # This enables "Configuration over Convention" - no Python code changes needed.
  #
  # Format:
  #   skill.command:
  #     alias: "native_name"          # Rename the tool (optional)
  #     append_doc: "..."             # Behavioral instructions (optional)
  #
  # The append_doc content is injected into the tool description and serves as
  # a context-embedded prompt that Claude reads when deciding which tool to use.

  overrides:
    # Web Fetching - Critical override to prevent curl/wget usage
    crawl4ai.crawl_url:
      alias: "webCrawl"
      append_doc: |

        ## ⚡️ OPERATIONAL RULE
        - **PRIMARY TOOL**: Use this tool for ALL web content retrieval.
        - **FORBIDDEN**: Do NOT use `curl`, `wget`, or shell requests.
        - **OUTPUT**: Returns structured Markdown, optimized for reading.

    # Memory Operations
    memory.get_memory_stats:
      alias: "get_memory_stats_rename"
      append_doc: "\n\nSemantic search across your project's memory/knowledge base."
    memory.save_memory:
      alias: "save_memory"
      append_doc: "\n\nPersist important context for future sessions."

# =============================================================================
# Claude Code Symbiosis - Context Compression
# =============================================================================
# Settings for Dynamic Context Compression when injecting context to Claude Code
# Prevents context bloat by summarizing large RAG results

context_compression:
  # Enable/disable context compression globally
  enabled: true

  # Maximum context tokens before compression is triggered
  # Claude 3.5 Sonnet has ~200K context window, but we compress to save tokens
  max_context_tokens: 4000

  # Maximum file size (KB) before compression is triggered
  # Files larger than this will be summarized instead of fully included
  max_file_size_kb: 50

  # Compression method: "llm" (use LLM to summarize) or "truncate" (simple cut)
  method: "llm"

# =============================================================================
# Router Configuration
# =============================================================================
# The Grand Unified Router - Hybrid semantic + keyword search
router:
  # Cache settings
  cache:
    max_size: 1000 # Maximum cache entries
    ttl: 300 # Cache TTL in seconds

  # Search weights (semantic + keyword fusion)
  search:
    # Profile selection behavior.
    # Runtime picks a profile automatically (LLM-assisted when available),
    # then falls back to active_profile.
    active_profile: "balanced"
    auto_profile_select: true
    profiles:
      balanced:
        high_threshold: 0.75
        medium_threshold: 0.50
        high_base: 0.90
        high_scale: 0.05
        high_cap: 0.99
        medium_base: 0.60
        medium_scale: 0.30
        medium_cap: 0.89
        low_floor: 0.10
      precision:
        high_threshold: 0.82
        medium_threshold: 0.58
        high_base: 0.92
        high_scale: 0.04
        high_cap: 0.99
        medium_base: 0.62
        medium_scale: 0.24
        medium_cap: 0.88
        low_floor: 0.10
      recall:
        high_threshold: 0.68
        medium_threshold: 0.42
        high_base: 0.88
        high_scale: 0.06
        high_cap: 0.99
        medium_base: 0.56
        medium_scale: 0.35
        medium_cap: 0.90
        low_floor: 0.08

    # Defaults used by `omni route test` when flags are not provided.
    default_limit: 10
    default_threshold: 0.0 # Show all results (no confidence filtering)
    rerank: true # Canonical rerank toggle for router hybrid pipeline

    semantic_weight: 0.7 # Weight for semantic search
    keyword_weight: 0.3 # Weight for keyword search
    adaptive_threshold_step: 0.15 # Threshold reduction step per retry in adaptive search
    adaptive_max_attempts: 3 # Max adaptive attempts before returning best available
    schema_file: "packages/shared/schemas/omni.router.search_config.v1.schema.json" # SSOT: relative to project root; or absolute path

    # Intent terms for overlap boost (data-driven). Tools with routing_keywords/intents matching these get a boost when they appear in the query. Empty = use built-in set. Scale: add domain terms here.
    intent_vocab: [] # e.g. ["research", "analyze", "crawl", "commit", "search", "find", "recall", "save"]

  # Sniffer settings (context activation from filesystem + dynamic sniffers)
  sniffer:
    score_threshold: 0.5 # Dynamic sniffer activation threshold [0.0, 1.0]

  # Query translation: non-English → English before routing. Default on (we don't know user language; common language is English).
  # This layer is part of the search pipeline; it also feeds into a stronger search (e.g. enrichment). LLM only; no rule-based fallback.
  translation:
    enabled: true # Default on. Set false to disable (e.g. when all queries are known to be English).
    # model: null  # Optional: model for translation only. Else uses inference.model
    fallback_to_original: true # On LLM failure, use original query

  # Catalog enrichment: diversify attribute values (e.g. routing_keywords) via LLM for more precise search.
  enrichment:
    enabled: false # Set true to expand routing_keywords with synonyms/related terms at index time
    expand_keywords: true # When enabled, LLM suggests extra keywords per tool (merged into index)
    # model: null  # Optional: model for enrichment only. Else uses inference.model

  # Optional LLM-driven intent classification for agentic search (P2). When enabled, intent is computed by LLM; otherwise rule-based (exact vs hybrid + file_discovery).
  intent:
    use_llm: false # Set true to use LLM for intent (exact | semantic | hybrid) and category_filter; fallback to rule-based on failure
    # model: null  # Optional: model for intent only. Else uses inference.model

  # Query normalization for routing (data-driven; extend here instead of code). Typo map: typo -> correct.
  # Hybrid search + relationship graph do the heavy lifting; this only fixes token-level noise.
  normalize:
    typos:
      analzye: analyze
      reserach: research

# =============================================================================
# Claude Code Symbiosis - Post-Mortem Audit
# =============================================================================
# Settings for automatic review after Claude Code execution

post_mortem:
  # Enable/disable Post-Mortem audit after Claude Code execution
  enabled: true

  # Confidence threshold: audits below this are flagged for manual review
  confidence_threshold: 0.8

# =============================================================================
# LLM Context Limits
# =============================================================================
context:
  max_schema_tokens: 4000 # Max tokens for tool schemas
  avg_schema_tokens: 250 # Est. tokens per tool schema

security:
  # Enable/disable security scanning (default: true)
  enabled: true

  # Thresholds for security decisions (0-100)
  # Score >= block_threshold: BLOCK the skill
  # Score >= warn_threshold: WARN user but allow
  block_threshold: 30
  warn_threshold: 10

  # Trusted sources (skip security scan for these)
  # Format: "github.com/username" or full repository URL patterns
  trusted_sources:
    - "github.com/omni-dev"
    - "github.com/trusted-org"

  # Sandbox configuration (future enhancement)
  sandbox:
    enabled: false
    timeout_seconds: 30
    memory_limit_mb: 128
